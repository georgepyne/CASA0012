{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import datetime, timedelta\n",
    "from geopandas import GeoDataFrame\n",
    "import geopandas as gpd\n",
    "import gpd_lite_toolbox as glt\n",
    "import pointpats.quadrat_statistics as qs\n",
    "from pointpats import PointPattern\n",
    "import pysal as ps\n",
    "import shapely.speedups\n",
    "import pandas as pd\n",
    "from sklearn.cluster import DBSCAN, OPTICS\n",
    "import shapely.geometry\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "import warnings\n",
    "import contextily as ctx\n",
    "import zipfile\n",
    "\n",
    "\n",
    "plt.rcParams[\"font.family\"] = \"Times New Roman\"\n",
    "shapely.speedups.enable()\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams[\"figure.figsize\"] = [12.0, 12.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_basemap(ax, zoom, url='http://tile.stamen.com/terrain/tileZ/tileX/tileY.png'):\n",
    "    xmin, xmax, ymin, ymax = ax.axis()\n",
    "    basemap, extent = ctx.bounds2img(xmin, ymin, xmax, ymax, zoom=zoom, url=url)\n",
    "    ax.imshow(basemap, extent=extent, interpolation='bilinear')\n",
    "    # restore original x/y limits\n",
    "    ax.axis((xmin, xmax, ymin, ymax))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pip_count(points, polygon):\n",
    "    try : \n",
    "        pips = points.within(polygon).value_counts()[True]\n",
    "    except:\n",
    "        pips = 0\n",
    "    \n",
    "    return pips"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Prep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "zf = '/Users/GeorgePyne/Documents/CASA/Digital Visualisation/Group Project/Foursquare_Data.zip'\n",
    "csvs = [i for i in zipfile.ZipFile(zf).namelist() if str(i).endswith('csv')]\n",
    "with zipfile.ZipFile(zf) as zipf:\n",
    "    with zipf.open(csvs[0]) as myZip:\n",
    "        df = pd.read_csv(myZip)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import zipfile\n",
    "zf = '/Users/GeorgePyne/Downloads/dataset_tsmc2014.zip'\n",
    "csvs = [i for i in zipfile.ZipFile(zf).namelist() if str(i).endswith('csv')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk = df.loc[df['country_code']=='GB']\n",
    "df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "uk['geometry'] = [shapely.geometry.Point(lat,lon) for lat,lon in zip(uk.lat,uk.lon)]\n",
    "uk = GeoDataFrame(uk)\n",
    "# pd.to_datetime(uk.datetime)\n",
    "uk = GeoDataFrame(uk)\n",
    "uk.crs = {'init': 'epsg:4326'}\n",
    "uk = uk.to_crs(epsg=27700)\n",
    "\n",
    "bb = gpd.read_file('/Users/GeorgePyne/Documents/Geography/2nd Year/Spatial Analysis/5SSG2060_CW1_1531870/data/LDN-LSOAs/LDN-LSOAs.shp')\n",
    "bb = bb['geometry']\n",
    "bb.crs = {'init': 'epsg:27700'}\n",
    "\n",
    "xmin, ymin, xmax, ymax = bb.total_bounds\n",
    "lon = uk.cx[xmin:xmax, ymin:ymax]\n",
    "uk = None\n",
    "lon['datetime'] = pd.to_datetime(lon['datetime'])\n",
    "lon = lon.reset_index(drop=True)\n",
    "lon['lon'],lon['lat'] = lon['geometry'].apply(lambda x: x.x), lon['geometry'].apply(lambda x: x.y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lon.groupby('venue_type').count().sort_values(by='user_id', ascending=False).head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base = lon.datetime[0]\n",
    "lon['time'] = lon.datetime - base\n",
    "lon = lon.sort_values(by='time')\n",
    "lon['time'] = lon['time'].apply(lambda x: (x.total_seconds() / 60)).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decomp_preperation(df):\n",
    "    \"\"\"Prepares the data for ST decomposition in XYT format\"\"\"\n",
    "    filename = input('Filename? ')\n",
    "    directory = \"/Users/GeorgePyne/Documents/CASA/Dissertation/IPYNB/decompSpaceTime-master/files/\"+filename\n",
    "    \n",
    "    df[['lon','lat','time']].to_csv(directory,header=False,index=False)\n",
    "    xmin, xmax = df.lon.min(), df.lon.max() \n",
    "    ymin, ymax = df.lat.min(), df.lat.max()\n",
    "    zmin, zmax = df.time.min(), df.time.max()\n",
    "        \n",
    "    src = open(directory,\"r\")\n",
    "    fline = '{0},{1},{2},{3},{4},{5}\\n'.format(xmin,xmax,ymin,ymax,zmin,zmax)\n",
    "    oline = src.readlines()\n",
    "    oline.insert(0,fline)\n",
    "    src.close()\n",
    "    \n",
    "    src=open(directory,\"w\")\n",
    "    src.writelines(oline)\n",
    "    src.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "decomp_preperation(lon)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spatiotemporal Decomposition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pList"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import modules\n",
    "from datetime import datetime\n",
    "import sys, os\n",
    "import decomposition as decomp, settings as sett\n",
    "import geopandas as gpd\n",
    "import pandas as pd\n",
    "import shapely\n",
    "from shapely import geometry\n",
    "import shutil\n",
    "\n",
    "#set recursion limit\n",
    "sys.setrecursionlimit(8000)\n",
    "\n",
    "#initialize global variables\n",
    "sett.init()\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#read parameters\n",
    "pFile = open('files/parameterFile.txt', \"r\")\n",
    "pFile.readline()\n",
    "pList = pFile.readline().split(\"\\t\")\n",
    "\n",
    "sett.p1 = float(pList[0])\t# p1 = spatial bandwidth\n",
    "sett.p2 = float(pList[1])\t# p2 = temporal bandwidth\n",
    "sett.p3 = float(pList[2])\t# p3 = spatial resolution\n",
    "sett.p4 = float(pList[3])\t# p4 = temporal resolution\n",
    "sett.p5 = float(pList[4])\t# p5 = number of points threshold (T1)\n",
    "sett.p6 = float(pList[5])\t# p6 = buffer ratio threshold (T2)\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#create output directory\n",
    "sett.dir1 = 'pointFiles'\n",
    "sett.dir2 = 'timeFiles'\n",
    "sett.dir3 = 'shpFiles'\n",
    "\n",
    "if os.path.exists(sett.dir1):\n",
    "    shutil.rmtree(sett.dir1)\n",
    "os.makedirs(sett.dir1)\n",
    "\n",
    "if os.path.exists(sett.dir2):\n",
    "    shutil.rmtree(sett.dir2)\n",
    "os.makedirs(sett.dir2)\n",
    "\n",
    "if os.path.exists(sett.dir3):\n",
    "    shutil.rmtree(sett.dir3)\n",
    "os.makedirs(sett.dir3)\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#read input point file\n",
    "pFile = open('files/decomp_test_prep.txt', \"r\")\n",
    "inX, inY, inZ = [], [], []\n",
    "r = pFile.readline().split(\",\")\n",
    "xmin, xmax, ymin, ymax, zmin, zmax = float(r[0]), float(r[1]), float(r[2]), float(r[3]), float(r[4]), float(r[5].strip())\n",
    "\n",
    "for record in pFile:   \n",
    "    inX.append(float(record.split(\",\")[0]))\n",
    "    inY.append(float(record.split(\",\")[1]))\n",
    "    inZ.append(float(record.split(\",\")[2]))\n",
    "    \n",
    "pFile.close()\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
    "#start decomposition\n",
    "startTime = datetime.now()\n",
    "decomp.decompose(inX, inY, inZ, xmin, xmax, ymin, ymax, zmin, zmax)\n",
    "endTime = datetime.now()\n",
    "\n",
    "#record decomposition time\n",
    "runTime = endTime - startTime \n",
    "print(\"Decomposition into {0} files in {1}.\".format(len(os.listdir('pointFiles')), runTime))\n",
    "tFile=open('timeFiles/decomp_time.txt', \"w\")\n",
    "tFile.write(str(runTime))\n",
    "tFile.close()\n",
    "#---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def spacetime_cluster_decomposition(directory):\n",
    "    \"\"\"\n",
    "    A function to extract moving and static cluster frames \n",
    "    and save their polygonal geometry to a time-based STR Tree\n",
    "    \n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cluster_spots, cluster_time = [],[]\n",
    "static_frames, moving_frames = [], []\n",
    "static_frames_t, moving_frames_t = [], []\n",
    "lm_list = []\n",
    "lm_moves_list = []\n",
    "# txts = []\n",
    "times = []\n",
    "moving_frame_t = []\n",
    "\n",
    "directory = '/Users/GeorgePyne/Documents/CASA/Dissertation/IPYNB/decompSpaceTime-master/pointFiles'\n",
    "\n",
    "for filename in os.listdir(directory)[0:3]: # Iterate over each decomposition domain\n",
    "    txt = pd.read_csv(directory+'/'+filename, skiprows=1, names=['lon','lat','time']) # read decomp\n",
    "#     txts.append(txt)\n",
    "    points = [[lat,lon] for lat,lon in zip(txt.lat, txt.lon)] # Save point patter to test global CSR\n",
    "    pp = PointPattern(points) # Create point pattern object\n",
    "    domain_quadrat = qs.QStatistic(pp, shape= \"rectangle\", nx = 6, ny = 7) # Run quadrat analysis on point pattern object\n",
    "#     domain_quadrat.plot()\n",
    "    pv = float(str(domain_quadrat.chi2_pvalue)[0:4])\n",
    "    if pv < 1.0: # If not CSR and if statistically significant\n",
    "        start_time = datetime.now() # Time subdomain process\n",
    "        \n",
    "        frames = []\n",
    "        counts = []\n",
    "\n",
    "        # Create geo_df of points to rasterize\n",
    "        points_gdf = GeoDataFrame([shapely.geometry.Point(point) for point in points]).rename(columns={0:'geometry'})\n",
    "        xmin,ymin,xmax,ymax = points_gdf.total_bounds # Get point bounds\n",
    "        height = (xmax-xmin)/ 6 # Set grid resolution\n",
    "        grid = glt.make_grid(points_gdf,height, False) # Create unclipped grid   \n",
    "        \n",
    "        \n",
    "        for t in txt.time.unique(): # Iterate over each frame\n",
    "            frame = txt.loc[txt.time == t] # create frame of each time step\n",
    "            points = [[lat,lon] for lat,lon in zip(frame.lat, frame.lon)] # make list of points\n",
    "            frame['geometry'] = [shapely.geometry.Point(point) for point in points] # make point df\n",
    "            frame = GeoDataFrame(frame) # change to gdf\n",
    "            frames.append(frame)\n",
    "            grid[str(int(t))+'_count'] = grid.geometry.apply(lambda x: pip_count(frame,x)) # Aggregate points to grid\n",
    "\n",
    "        W = ps.weights.Queen.from_dataframe(grid) # Calculate spatial Queen contiguity weights object from grid gdf \n",
    "        tci = np.array(grid[grid.columns[1:]]) # Save transitions as matrix\n",
    "        lm = ps.LISA_Markov(tci,W) # Calculate LISA markov transitions from W and matrix\n",
    "        lm_list.append(lm)\n",
    "        \n",
    "        # creat a LISA transition df of observed against expected\n",
    "        lm_moves = pd.DataFrame({\"Transitions\":lm.transitions.flatten(), \"Expected\":lm.expected_t.flatten()}) \n",
    "        lm_moves['Transitions'] = lm_moves['Transitions'].astype(int)\n",
    "        lm_moves.index = [\"HH-HH\",\"HH-LH\",\"HH-LL\",\"HH-HL\", # Make index 16 possible move types\n",
    "                  \"LH-HH\",\"LH-LH\",\"LH-LL\",\"LH-HL\",\n",
    "                  \"LL-HH\",\"LL-LH\",\"LL-LL\",\"LL-HL\",\n",
    "                  \"HL-HH\",\"HL-LH\",\"HL-LL\",\"HL-HL\"]\n",
    "        lm_moves['Residuals'] = lm_moves.Transitions - lm_moves.Expected # Calculate difference of observed - expected\n",
    "        lm_moves_list.append(lm_moves)\n",
    "    \n",
    "        if lm.chi_2[1] < 0.05:\n",
    "            for frame in frames:                \n",
    "                if len(frame)>2:\n",
    "                    points = [[lat,lon] for lat,lon in zip(frame.lat, frame.lon)]\n",
    "                    pp_t = PointPattern(points)\n",
    "                    q_h_t = qs.QStatistic(pp,shape= \"rectangle\",nx = 6, ny = 7)\n",
    "                    pv_t = float(str(q_h_t.chi2_pvalue)[0:4])\n",
    "                    if pv_t < 1.0:\n",
    "                        eps = pp_t.mean_nnd            \n",
    "                        min_samples = int(len(frame) / 6)\n",
    "                    if min_samples < 3:\n",
    "                        min_samples = 3\n",
    "                    labels = OPTICS(eps=eps, min_samples=min_samples).fit(points).labels_\n",
    "                    frame['labels'] = labels\n",
    "                    try:\n",
    "                        for i in frame.labels.unique():\n",
    "                            if i > -1:\n",
    "                                if len(frame.loc[frame['labels']==i]) > 2:\n",
    "                                    moving_frames.append\\\n",
    "                                    (shapely.geometry.MultiPoint\\\n",
    "                                     ([shapely.geometry.Point(lat,lon)\\\n",
    "                                       for lat,lon in zip(frame.loc[frame['labels']==i].lon,\\\n",
    "                                                          frame.loc[frame['labels']==i].lat)]).convex_hull)\n",
    "                                    moving_frame_t.append(frame.time.unique()[0])\n",
    "                                    end_time = datetime.now() # Save endtime\n",
    "                                    run_time = end_time - start_time # Find runtime to adjudge parallelization\n",
    "                                    times.append(run_time)\n",
    "                    except:\n",
    "                        print(frame.head(4))\n",
    "                        \n",
    "        else:\n",
    "            for frame in frames:  \n",
    "                if len(frame)>4:\n",
    "                    points = [[lat,lon] for lat,lon in zip(frame.lat, frame.lon)]\n",
    "                    pp_t = PointPattern(points)\n",
    "                    q_h_t = qs.QStatistic(pp,shape= \"rectangle\",nx = 6, ny = 7)\n",
    "                    pv_t = float(str(q_h_t.chi2_pvalue)[0:4])\n",
    "                    if pv_t < 1.0:\n",
    "                        eps = pp_t.mean_nnd            \n",
    "                        min_samples = int(len(frame) / 6)\n",
    "                    if min_samples < 3:\n",
    "                        min_samples = 3\n",
    "                    labels = OPTICS(eps=eps, min_samples=min_samples).fit(points).labels_\n",
    "                    frame['labels'] = labels\n",
    "                    try:                        \n",
    "                        for i in frame.labels.unique():                            \n",
    "                            if i > -1:\n",
    "                                if len(frame.loc[frame['labels']==i]) > 2:                                   \n",
    "                                    static_frames.append(shapely.geometry.MultiPoint([shapely.geometry.Point(lat,lon) for lat,lon in zip(frame.loc[frame['labels']==i].lon, frame.loc[frame['labels']==i].lat)]).convex_hull)\n",
    "                                    static_frames_t.append(frame.time.unique()[0])\n",
    "                                    end_time = datetime.now() # Save endtime\n",
    "                                    run_time = end_time - start_time # Find runtime to adjudge parallelization\n",
    "                                    times.append(run_time)\n",
    "                    except:\n",
    "                        print(frame.head(4))\n",
    "                \n",
    "        end_time = datetime.now()\n",
    "        run_time = end_time - start_time\n",
    "        times.append(run_time)\n",
    "    else:\n",
    "        print(\"CSR expected in {}.\".format(filename))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "txt = txt.sort_values(by='time')\n",
    "txt['framerate'] = txt.time.shift(1)\n",
    "frame_rate = txt[['time','framerate']].diff().framerate.mean()\n",
    "txt['framerate'] = (txt.framerate / frame_rate).fillna(method='backfill').astype(int)\n",
    "txt.time = txt.framerate * frame_rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_run_times(times):\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import geopandas as gpd\n",
    "bb = gpd.read_file('/Users/GeorgePyne/Documents/Geography/2nd Year/Spatial Analysis/5SSG2060_CW1_1531870/data/LDN-LSOAs/LDN-LSOAs.shp')\n",
    "bb = bb['geometry']\n",
    "bb.crs = {'init': 'epsg:27700'}\n",
    "bb = bb.to_crs(epsg=3857)\n",
    "xmin, ymin, xmax, ymax = bb.total_bounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_clusters['minutes'] = moving_clusters.time * 30\n",
    "moving_clusters['datetime'] = moving_clusters.minutes.apply(lambda x: ts + timedelta(minutes=x))\n",
    "moving_clusters = moving_clusters.sort_values(by='time', ascending=True)\n",
    "moving_clusters.crs = {'init': 'epsg:27700'}\n",
    "moving_clusters = moving_clusters.to_crs(epsg=3857)\n",
    "for time in moving_clusters.time.unique():\n",
    "    ax = moving_clusters.loc[moving_clusters['time']==time].plot(cmap='OrRd')\n",
    "    ax.set_xlim(xmin,xmax)\n",
    "    ax.set_ylim(ymin,ymax)\n",
    "    ax.set_axis_off()\n",
    "    add_basemap(ax, zoom=13, url=ctx.sources.ST_TONER_LITE)\n",
    "#     plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_clusters = GeoDataFrame(moving_frames).rename(columns={0:'geometry'})\n",
    "moving_clusters['time'] = moving_frame_t\n",
    "moving_clusters.time.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LISA_markov_diagnostics(lm_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def LISA_markov_diagnostics(lm_list):\n",
    "    \n",
    "    \n",
    "    for lm in lm_list:\n",
    "#         lm_moves = pd.DataFrame({\"Transitions\":lm.transitions.flatten(), \"Expected\":lm.expected_t.flatten()}) \n",
    "#         lm_moves['Transitions'] = lm_moves['Transitions'].astype(int)\n",
    "#         lm_moves.index = [\"HH-HH\",\"HH-LH\",\"HH-LL\",\"HH-HL\", # Make index 16 possible move types\n",
    "#                   \"LH-HH\",\"LH-LH\",\"LH-LL\",\"LH-HL\",\n",
    "#                   \"LL-HH\",\"LL-LH\",\"LL-LL\",\"LL-HL\",\n",
    "#                   \"HL-HH\",\"HL-LH\",\"HL-LL\",\"HL-HL\"]\n",
    "#         lm_moves['Residuals'] = lm_moves.Transitions - lm_moves.Expected # Calculate difference of observed - expected\n",
    "#         print('Sum of LM transitions:')\n",
    "#         print(sum([lm_moves['Residuals'][0], lm_moves['Residuals'][3],lm_moves['Residuals'][8],lm_moves['Residuals'][15]]))\n",
    "        \n",
    "        print(\"Chi2: %8.3f, p: %5.2f, dof: %d\" % lm.chi_2)\n",
    "#         ax = sns.heatmap(np.matrix([lm_moves.Transitions[:4].values,\\\n",
    "#         lm_moves.Transitions[4:8].values,\\\n",
    "#         lm_moves.Transitions[8:12].values,\\\n",
    "#         lm_moves.Transitions[12:].values]), cmap='Purples', \n",
    "#                     annot=True, annot_kws={'size':14}, cbar_kws={'label':\"LISA Markov observed transitions\"})\n",
    "#         ax.figure.axes[-1].yaxis.label.set_size(18)\n",
    "#         ax.figure.axes[-1].yaxis.label.set_size(18)\n",
    "#         ax.set_yticklabels(labels=['HH', 'LH', 'LL', 'HL'], size=18);\n",
    "#         ax.set_xticklabels(labels=['HH', 'LH', 'LL', 'HL'], size=18);\n",
    "#         plt.ylabel(\"Transition from:\", size=18)\n",
    "#         plt.xlabel(\"Transition to:\", size=18,);\n",
    "#         plt.show();\n",
    "\n",
    "#         ax = sns.heatmap(np.matrix([lm_moves.Residuals[:4].values,\\\n",
    "#         lm_moves.Residuals[4:8].values,\\\n",
    "#         lm_moves.Residuals[8:12].values,\\\n",
    "#         lm_moves.Residuals[12:].values]), cmap='coolwarm', \n",
    "#                     annot=True, annot_kws={'size':14}, cbar_kws={'label':\"LISA Markov observed – expected transitions\"})\n",
    "#         ax.figure.axes[-1].yaxis.label.set_size(18)\n",
    "#         ax.set_yticklabels(labels=['HH', 'LH', 'LL', 'HL'], size=18);\n",
    "#         ax.set_xticklabels(labels=['HH', 'LH', 'LL', 'HL'], size=18);\n",
    "#         plt.ylabel(\"Transition from:\", size=18)\n",
    "#         plt.xlabel(\"Transition to:\", size=18);\n",
    "#         plt.show();\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "moving_clusters = GeoDataFrame(cluster_spots).rename(columns={0:'geometry'})\n",
    "moving_clusters['hour'] = cluster_time\n",
    "moving_clusters['geom_type'] = moving_clusters.geometry.apply(lambda x: type(x))\n",
    "moving_clusters = moving_clusters.loc[moving_clusters['geom_type']==type(test)][['hour','geometry']]\n",
    "for hour in moving_clusters.hour.unique():\n",
    "    moving_clusters.loc[moving_clusters['hour']==hour].to_file('/Users/GeorgePyne/Desktop/shptest/{}hourclustertest.shp'.format(str(hour)), driver='ESRI Shapefile')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xmin,ymin,xmax,ymax = gdf_points.total_bounds # Get point bounds\n",
    "height = (xmax-xmin)/ 6 # Set grid resolution\n",
    "grid = glt.make_grid(gdf_points,height, False) # Create unclipped grid\n",
    "grid['counts'] = grid.geometry.apply(lambda x: pip_count(gdf_points,x)) # Aggregate points to grid"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
